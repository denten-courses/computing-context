{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Summary and Sentiment\n",
    "\n",
    "## Screencasts\n",
    "\n",
    "https://drive.google.com/drive/u/1/folders/0B4OAOue0b3VMOU1yYW1JcUlNcWM\n",
    "\n",
    "## Readings\n",
    "\n",
    "\n",
    "## Home Experiment\n",
    "\n",
    "\n",
    "## Lecture Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "went\n",
      "is\n",
      "wa\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# lemma\n",
    "new_blob = TextBlob(\"going went is was be\")\n",
    "for word in new_blob.words:\n",
    "    print word.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "be\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# oh-oh problem! \n",
    "for word in new_blob.words:\n",
    "    print word.lemmatize(\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word.lemmatize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmas need part of speech!\n",
    "import nltk\n",
    "\n",
    "# runs nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', u'DT'),\n",
       " ('titular', u'JJ'),\n",
       " ('threat', u'NN'),\n",
       " ('of', u'IN'),\n",
       " ('The', u'DT'),\n",
       " ('Blob', u'NNP'),\n",
       " ('has', u'VBZ'),\n",
       " ('always', u'RB'),\n",
       " ('struck', u'VBN'),\n",
       " ('me', u'PRP'),\n",
       " ('as', u'IN'),\n",
       " ('the', u'DT'),\n",
       " ('ultimate', u'JJ'),\n",
       " ('movie', u'NN'),\n",
       " ('monster', u'NN'),\n",
       " ('an', u'DT'),\n",
       " ('insatiably', u'RB'),\n",
       " ('hungry', u'JJ'),\n",
       " ('amoeba-like', u'JJ'),\n",
       " ('mass', u'NN'),\n",
       " ('able', u'JJ'),\n",
       " ('to', u'TO'),\n",
       " ('penetrate', u'VB'),\n",
       " ('virtually', u'RB'),\n",
       " ('any', u'DT'),\n",
       " ('safeguard', u'NN'),\n",
       " ('capable', u'JJ'),\n",
       " ('of', u'IN'),\n",
       " ('as', u'IN'),\n",
       " ('a', u'DT'),\n",
       " ('doomed', u'JJ'),\n",
       " ('doctor', u'NN'),\n",
       " ('chillingly', u'RB'),\n",
       " ('describes', u'VBZ'),\n",
       " ('it', u'PRP'),\n",
       " ('assimilating', u'VBG'),\n",
       " ('flesh', u'NN'),\n",
       " ('on', u'IN'),\n",
       " ('contact', u'NN'),\n",
       " ('Snide', u'JJ'),\n",
       " ('comparisons', u'NNS'),\n",
       " ('to', u'TO'),\n",
       " ('gelatin', u'VB'),\n",
       " ('be', u'VB'),\n",
       " ('damned', u'VBN'),\n",
       " ('it', u'PRP'),\n",
       " (\"'s\", u'VBZ'),\n",
       " ('a', u'DT'),\n",
       " ('concept', u'NN'),\n",
       " ('with', u'IN'),\n",
       " ('the', u'DT'),\n",
       " ('most', u'RBS'),\n",
       " ('devastating', u'JJ'),\n",
       " ('of', u'IN'),\n",
       " ('potential', u'JJ'),\n",
       " ('consequences', u'NNS'),\n",
       " ('not', u'RB'),\n",
       " ('unlike', u'IN'),\n",
       " ('the', u'DT'),\n",
       " ('grey', u'NN'),\n",
       " ('goo', u'NN'),\n",
       " ('scenario', u'NN'),\n",
       " ('proposed', u'VBN'),\n",
       " ('by', u'IN'),\n",
       " ('technological', u'JJ'),\n",
       " ('theorists', u'NNS'),\n",
       " ('fearful', u'NN'),\n",
       " ('of', u'IN'),\n",
       " ('artificial', u'JJ'),\n",
       " ('intelligence', u'NN'),\n",
       " ('run', u'NN'),\n",
       " ('rampant', u'NN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagging is hard. this will take a while.\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = new_blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of tuples\n",
    "type(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list_item[1]'\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0d0e3bbf779b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlist_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_blob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlist_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list_item[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/textblob/decorators.pyc\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingCorpusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "# hmmm, why do you think is this not working?\n",
    "# look up wordnet vs. treebank parts of speech\n",
    "for list_item in new_blob.tags:\n",
    "    list_item[0].lemmatize('list_item[1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# booooring\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG v\n",
      "VBD v\n",
      "VBZ v\n",
      "VBD v\n",
      "VB v\n"
     ]
    }
   ],
   "source": [
    "lemmas = ()\n",
    "# ok this seems to work\n",
    "for list_item in new_blob.tags:\n",
    "    word = list_item[0]\n",
    "    tpos = list_item[1]\n",
    "    wpos = get_wordnet_pos(tpos)\n",
    "    print tpos, wpos\n",
    "    \n",
    "    #lemmas.append(list_item[0].lemmatize(get_wordent_pos(list_item[1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'go', u'go', u'be', u'be', 'be']\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "# lets make it better\n",
    "for list_item in new_blob.tags:\n",
    "    word = list_item[0]\n",
    "    tpos = list_item[1]\n",
    "    wpos = get_wordnet_pos(tpos)\n",
    "    lemmas.append(word.lemmatize(wpos))\n",
    "    \n",
    "print lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is about:\n",
      "tables\n",
      "cottons\n",
      "places\n",
      "exchanges\n",
      "covers\n"
     ]
    }
   ],
   "source": [
    "# ported from Allison Parish's excellent\n",
    "# http://rwet.decontextualize.com/book/textblob/\n",
    "\n",
    "# let's make a simple text summary\n",
    "from textblob import TextBlob, Word\n",
    "import random\n",
    "\n",
    "# From Tender Buttons by Gertrude Stein\n",
    "text = '''\n",
    "A cushion has that cover. Supposing you do not like to change, supposing it is very clean \n",
    "that there is no change in appearance, supposing that there is regularity and a costume is \n",
    "that any the worse than an oyster and an exchange. Come to season that is there any extreme \n",
    "use in feather and cotton. Is there not much more joy in a table and more chairs and very \n",
    "likely roundness and a place to put them. A circle of fine card board and a chance to see a tassel. \n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "nouns = list()\n",
    "for word, tag in blob.tags:\n",
    "    if tag == 'NN':\n",
    "        nouns.append(word.lemmatize())\n",
    "\n",
    "print \"This text is about:\"\n",
    "for item in random.sample(nouns, 5):\n",
    "    word = Word(item)\n",
    "    print word.pluralize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
